// API callback
Tabletop.singleton.loadSheet({"version":"1.0","encoding":"UTF-8","feed":{"xmlns":"http://www.w3.org/2005/Atom","xmlns$openSearch":"http://a9.com/-/spec/opensearchrss/1.0/","xmlns$gsx":"http://schemas.google.com/spreadsheets/2006/extended","id":{"$t":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od6/public/values"},"updated":{"$t":"2016-07-28T13:41:55.267Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"Fall_2013"},"link":[{"rel":"alternate","type":"application/atom+xml","href":"https://docs.google.com/spreadsheets/d/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/pubhtml"},{"rel":"http://schemas.google.com/g/2005#feed","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od6/public/values"},{"rel":"http://schemas.google.com/g/2005#post","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od6/public/values"},{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od6/public/values?alt\u003djson-in-script"}],"author":[{"name":{"$t":"eric.e.monson"},"email":{"$t":"eric.e.monson@gmail.com"}}],"openSearch$totalResults":{"$t":"14"},"openSearch$startIndex":{"$t":"1"},"entry":[{"id":{"$t":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od6/public/values/cokwr"},"updated":{"$t":"2016-07-28T13:41:55.267Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"9/6/2013"},"content":{"type":"text","$t":"speaker: Angela Zoss, affiliation: Duke Data Visualization Coordinator, title: Visualization for Teaching and Research (A Conference Report), abstract: The 2013 Gordon Research Conference on Visualization in Science \u0026 Education brought together a diverse group of researchers and practitioners to survey and problematize the use of visualizations for educational and research purposes.  Gordon Conferences are notable for having a unique format and approach; they are single track and restricted to a small group of attendees in the hopes of encouraging interdisciplinary discussions and collaborations.  In this talk, I will summarize some of the most exciting approaches presented at the conference and suggest how theoretical and technical advances can improve our design and use of visualizations.  The conference also inspired a new collaborative initiative that I am excited to present and discuss with the group., video: http://compsci.capture.duke.edu/Panopto/Pages/Viewer/Default.aspx?id\u003da20de379-5fd3-45ac-96a4-bc79bb959a40, slides: slides/2013VFF-GRC-Zoss.pdf, slidesnotes: PDF"},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od6/public/values/cokwr"}],"gsx$date":{"$t":"9/6/2013"},"gsx$speaker":{"$t":"Angela Zoss"},"gsx$affiliation":{"$t":"Duke Data Visualization Coordinator"},"gsx$title":{"$t":"Visualization for Teaching and Research (A Conference Report)"},"gsx$abstract":{"$t":"The 2013 Gordon Research Conference on Visualization in Science \u0026 Education brought together a diverse group of researchers and practitioners to survey and problematize the use of visualizations for educational and research purposes.  Gordon Conferences are notable for having a unique format and approach; they are single track and restricted to a small group of attendees in the hopes of encouraging interdisciplinary discussions and collaborations.  In this talk, I will summarize some of the most exciting approaches presented at the conference and suggest how theoretical and technical advances can improve our design and use of visualizations.  The conference also inspired a new collaborative initiative that I am excited to present and discuss with the group."},"gsx$video":{"$t":"http://compsci.capture.duke.edu/Panopto/Pages/Viewer/Default.aspx?id\u003da20de379-5fd3-45ac-96a4-bc79bb959a40"},"gsx$videonotes":{"$t":""},"gsx$slides":{"$t":"slides/2013VFF-GRC-Zoss.pdf"},"gsx$slidesnotes":{"$t":"PDF"},"gsx$livestream":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od6/public/values/cpzh4"},"updated":{"$t":"2016-07-28T13:41:55.267Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"9/13/2013"},"content":{"type":"text","$t":"speaker: Cory Quammen, affiliation: UNC-CH Computer Science, title: The Virtual Pediatric Airways Workbench, abstract: Upper airway problems in young children may lead to life threatening respiratory difficulties, poor growth, aspiration, delay in speech development and long-term illness. Treatment of these children is typically directed by the clinician's experience and preference, rather than by published protocols or quantitative measures of airway physiology and anatomy. Improved methods of evaluating and determining best treatment options would benefit clinical care and outcomes. At UNC, we are building the Virtual Pediatric Airways Workbench (VPAW) as a unified interface to tie airway segmentation algorithms, geometric modeling, and fluid dynamics simulations together into one system that a physician can use. The end goal is to provide a tool for use by physicians to make more informed decisions about clinical interventions. VPAW consists of four main components: segmentation, geometry editing, simulation, and visualization. It uses stereo display and haptic feedback to improve the surgical modeling user experience. In this talk, I will discuss these components and report initial results using it to model airflow on patients with subglottic stenosis, a narrowing of the airway below the vocal cords., video: http://compsci.capture.duke.edu/Panopto/Pages/Viewer/Default.aspx?id\u003d03c50774-fca2-41eb-8c9f-633290ebbbd4"},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od6/public/values/cpzh4"}],"gsx$date":{"$t":"9/13/2013"},"gsx$speaker":{"$t":"Cory Quammen"},"gsx$affiliation":{"$t":"UNC-CH Computer Science"},"gsx$title":{"$t":"The Virtual Pediatric Airways Workbench"},"gsx$abstract":{"$t":"Upper airway problems in young children may lead to life threatening respiratory difficulties, poor growth, aspiration, delay in speech development and long-term illness. Treatment of these children is typically directed by the clinician's experience and preference, rather than by published protocols or quantitative measures of airway physiology and anatomy. Improved methods of evaluating and determining best treatment options would benefit clinical care and outcomes. At UNC, we are building the Virtual Pediatric Airways Workbench (VPAW) as a unified interface to tie airway segmentation algorithms, geometric modeling, and fluid dynamics simulations together into one system that a physician can use. The end goal is to provide a tool for use by physicians to make more informed decisions about clinical interventions. VPAW consists of four main components: segmentation, geometry editing, simulation, and visualization. It uses stereo display and haptic feedback to improve the surgical modeling user experience. In this talk, I will discuss these components and report initial results using it to model airflow on patients with subglottic stenosis, a narrowing of the airway below the vocal cords."},"gsx$video":{"$t":"http://compsci.capture.duke.edu/Panopto/Pages/Viewer/Default.aspx?id\u003d03c50774-fca2-41eb-8c9f-633290ebbbd4"},"gsx$videonotes":{"$t":""},"gsx$slides":{"$t":""},"gsx$slidesnotes":{"$t":""},"gsx$livestream":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od6/public/values/cre1l"},"updated":{"$t":"2016-07-28T13:41:55.267Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"9/20/2013"},"content":{"type":"text","$t":"speaker: Lee Sorensen, affiliation: Duke Visual Studies Librarian, title: How Do You Look? Visual Knowledge and Representation, abstract: Pictures (\"visual information\") require the same critical judgment texts do. Just as some texts don't tell the truth -- intentionally or unintentionally -- the same is true with images. As researchers seek to represent data in visual forms -- and as the public is evermore saturated with pictures to persuade opinion -- it's necessary to understand both how humans natively understand images and how the public can be educated for more objective conclusions to complex images. Approaching image representation as a tool rather than as an immutable result must be the goal of every visually literate person. Using the University's \"How Do You Look\" initiative for understanding visual information, this talk will address techniques in teaching visual literacy based on how they are understood and learned., video: http://compsci.capture.duke.edu/Panopto/Pages/Viewer/Default.aspx?id\u003d24f5fb60-be5e-400a-8822-768c00e23604"},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od6/public/values/cre1l"}],"gsx$date":{"$t":"9/20/2013"},"gsx$speaker":{"$t":"Lee Sorensen"},"gsx$affiliation":{"$t":"Duke Visual Studies Librarian"},"gsx$title":{"$t":"How Do You Look? Visual Knowledge and Representation"},"gsx$abstract":{"$t":"Pictures (\"visual information\") require the same critical judgment texts do. Just as some texts don't tell the truth -- intentionally or unintentionally -- the same is true with images. As researchers seek to represent data in visual forms -- and as the public is evermore saturated with pictures to persuade opinion -- it's necessary to understand both how humans natively understand images and how the public can be educated for more objective conclusions to complex images. Approaching image representation as a tool rather than as an immutable result must be the goal of every visually literate person. Using the University's \"How Do You Look\" initiative for understanding visual information, this talk will address techniques in teaching visual literacy based on how they are understood and learned."},"gsx$video":{"$t":"http://compsci.capture.duke.edu/Panopto/Pages/Viewer/Default.aspx?id\u003d24f5fb60-be5e-400a-8822-768c00e23604"},"gsx$videonotes":{"$t":""},"gsx$slides":{"$t":""},"gsx$slidesnotes":{"$t":""},"gsx$livestream":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od6/public/values/chk2m"},"updated":{"$t":"2016-07-28T13:41:55.267Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"9/27/2013"},"content":{"type":"text","$t":"speaker: David J. Zielinski, affiliation: Duke DiVE, title: Intercept Tags: Enhancing Intercept-based Systems, abstract: In a previous Friday Forum talk we discussed how we were able to utilize the well known technique of OpenGL intercept to display and interact with MATLAB simulations in the DiVE, our Virtual Reality system, through the ML2VR framework. In this talk we discuss our latest contributions to this topic, which will soon be presented at the ACM VRST conference. We present intercept tags, which are OpenGL geometries that are interpreted instead of rendered. We have identified and developed several uses for intercept tags, including hand-off interactions, display techniques, and visual enhancements. Our results show that intercept tags significantly improve user performance and experience., video: http://compsci.capture.duke.edu/Panopto/Pages/Viewer/Default.aspx?id\u003d65f692ea-a332-4edc-b254-b71195e7ba7f"},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od6/public/values/chk2m"}],"gsx$date":{"$t":"9/27/2013"},"gsx$speaker":{"$t":"David J. Zielinski"},"gsx$affiliation":{"$t":"Duke DiVE"},"gsx$title":{"$t":"Intercept Tags: Enhancing Intercept-based Systems"},"gsx$abstract":{"$t":"In a previous Friday Forum talk we discussed how we were able to utilize the well known technique of OpenGL intercept to display and interact with MATLAB simulations in the DiVE, our Virtual Reality system, through the ML2VR framework. In this talk we discuss our latest contributions to this topic, which will soon be presented at the ACM VRST conference. We present intercept tags, which are OpenGL geometries that are interpreted instead of rendered. We have identified and developed several uses for intercept tags, including hand-off interactions, display techniques, and visual enhancements. Our results show that intercept tags significantly improve user performance and experience."},"gsx$video":{"$t":"http://compsci.capture.duke.edu/Panopto/Pages/Viewer/Default.aspx?id\u003d65f692ea-a332-4edc-b254-b71195e7ba7f"},"gsx$videonotes":{"$t":""},"gsx$slides":{"$t":""},"gsx$slidesnotes":{"$t":""},"gsx$livestream":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od6/public/values/ciyn3"},"updated":{"$t":"2016-07-28T13:41:55.267Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"10/4/2013"},"content":{"type":"text","$t":"speaker: Christopher Healey, affiliation: NCSU Computer Science, title: Visualizing Tweet Sentiment, abstract: During this talk I will discuss a new project that focuses on ways to visualize text, specifically short text snippets like those found in tweets, SMS text messages, or Facebook wall posts. Our visualizations present text collections using a combination of numerous approaches: sentiment analysis, topic clusters, tag clouds, affinity graphs, volume timelines, and sentiment heatmaps. A second aspect of this project involves web-based visualization. We are implementing our visualization tools in Javascript, HTML, and CSS, allowing us to distribute our visualizations through any modern web browser, without the need for plug-ins. This also offers an opportunity to assess the strengths and limitations of current web-based visualization and user interface libraries.\n\u003c/p\u003e\u003cp class\u003d\"abstract\"\u003e\nWe chose Twitter as a testbed for our techniques. Twitter's publicly accessible APIs allow us to query collections of recent tweets for user-chosen keywords, or to tap into the real-time tweet stream \u0026ndash; the \"firehose\" \u0026ndash; to capture tweets by keyword as they are posted. To assess the practical usefulness and usability of our visualizations, we partnered with WRAL TV, the CBS/Fox network affiliate for the Raleigh, North Carolina broadcast region. WRAL ran our Twitter visualizations on their web site during the each of the recent U.S. Presidential debates. This allowed viewers to watch the debate, and at the same time to monitor the volume, sentiment, and content of tweets about the debate as they were posted in real-time. WRAL reporters used a modified version of the visualization tool to perform post-analysis of the captured tweet stream. Interesting findings were included in news stories they published following the debates.\n, video: http://compsci.capture.duke.edu/Panopto/Pages/Viewer/Default.aspx?id\u003d8f9d6cf5-1247-40f1-981a-02b808c1ff52, livestream: http://compsci.capture.duke.edu/Panopto/Pages/Viewer/Default.aspx?id\u003d8f9d6cf5-1247-40f1-981a-02b808c1ff52"},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od6/public/values/ciyn3"}],"gsx$date":{"$t":"10/4/2013"},"gsx$speaker":{"$t":"Christopher Healey"},"gsx$affiliation":{"$t":"NCSU Computer Science"},"gsx$title":{"$t":"Visualizing Tweet Sentiment"},"gsx$abstract":{"$t":"During this talk I will discuss a new project that focuses on ways to visualize text, specifically short text snippets like those found in tweets, SMS text messages, or Facebook wall posts. Our visualizations present text collections using a combination of numerous approaches: sentiment analysis, topic clusters, tag clouds, affinity graphs, volume timelines, and sentiment heatmaps. A second aspect of this project involves web-based visualization. We are implementing our visualization tools in Javascript, HTML, and CSS, allowing us to distribute our visualizations through any modern web browser, without the need for plug-ins. This also offers an opportunity to assess the strengths and limitations of current web-based visualization and user interface libraries.\n\u003c/p\u003e\u003cp class\u003d\"abstract\"\u003e\nWe chose Twitter as a testbed for our techniques. Twitter's publicly accessible APIs allow us to query collections of recent tweets for user-chosen keywords, or to tap into the real-time tweet stream \u0026ndash; the \"firehose\" \u0026ndash; to capture tweets by keyword as they are posted. To assess the practical usefulness and usability of our visualizations, we partnered with WRAL TV, the CBS/Fox network affiliate for the Raleigh, North Carolina broadcast region. WRAL ran our Twitter visualizations on their web site during the each of the recent U.S. Presidential debates. This allowed viewers to watch the debate, and at the same time to monitor the volume, sentiment, and content of tweets about the debate as they were posted in real-time. WRAL reporters used a modified version of the visualization tool to perform post-analysis of the captured tweet stream. Interesting findings were included in news stories they published following the debates.\n"},"gsx$video":{"$t":"http://compsci.capture.duke.edu/Panopto/Pages/Viewer/Default.aspx?id\u003d8f9d6cf5-1247-40f1-981a-02b808c1ff52"},"gsx$videonotes":{"$t":""},"gsx$slides":{"$t":""},"gsx$slidesnotes":{"$t":""},"gsx$livestream":{"$t":"http://compsci.capture.duke.edu/Panopto/Pages/Viewer/Default.aspx?id\u003d8f9d6cf5-1247-40f1-981a-02b808c1ff52"}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od6/public/values/ckd7g"},"updated":{"$t":"2016-07-28T13:41:55.267Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"10/11/2013"},"content":{"type":"text","$t":"speaker: Andrea Kleinsmith, affiliation: University of Florida Virtual Patients Group, title: Understanding and Designing with Users: From Recognizing Affect to Interacting with Virtual Humans, abstract: The number of technologies encountered by the average person on a day-to-day basis has increased considerably over the last decade. Hence, it is necessary to understand how users' interactions with technology can be improved. This understanding helps to increase the effectiveness of the technology itself, as well as improve users' experiences with technology. In this talk, I will highlight some of the results of my research which include: 1) building software that can automatically recognize users' affective states through their body expressions; 2) creating user interfaces that allow players to use their own body movements to customize the responses of virtual game characters; and 3) implementing virtual human technologies with an aim of training medical students in scenarios which are not always feasible with real people. I will conclude by discussing some of the issues and challenges that remain open in these research areas., video: http://compsci.capture.duke.edu/Panopto/Pages/Viewer/Default.aspx?id\u003d849ed9fb-b709-4dcc-b92b-c8cdd4bdb198, livestream: http://compsci.capture.duke.edu/Panopto/Pages/Viewer/Default.aspx?id\u003d849ed9fb-b709-4dcc-b92b-c8cdd4bdb198"},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od6/public/values/ckd7g"}],"gsx$date":{"$t":"10/11/2013"},"gsx$speaker":{"$t":"Andrea Kleinsmith"},"gsx$affiliation":{"$t":"University of Florida Virtual Patients Group"},"gsx$title":{"$t":"Understanding and Designing with Users: From Recognizing Affect to Interacting with Virtual Humans"},"gsx$abstract":{"$t":"The number of technologies encountered by the average person on a day-to-day basis has increased considerably over the last decade. Hence, it is necessary to understand how users' interactions with technology can be improved. This understanding helps to increase the effectiveness of the technology itself, as well as improve users' experiences with technology. In this talk, I will highlight some of the results of my research which include: 1) building software that can automatically recognize users' affective states through their body expressions; 2) creating user interfaces that allow players to use their own body movements to customize the responses of virtual game characters; and 3) implementing virtual human technologies with an aim of training medical students in scenarios which are not always feasible with real people. I will conclude by discussing some of the issues and challenges that remain open in these research areas."},"gsx$video":{"$t":"http://compsci.capture.duke.edu/Panopto/Pages/Viewer/Default.aspx?id\u003d849ed9fb-b709-4dcc-b92b-c8cdd4bdb198"},"gsx$videonotes":{"$t":""},"gsx$slides":{"$t":""},"gsx$slidesnotes":{"$t":""},"gsx$livestream":{"$t":"http://compsci.capture.duke.edu/Panopto/Pages/Viewer/Default.aspx?id\u003d849ed9fb-b709-4dcc-b92b-c8cdd4bdb198"}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od6/public/values/clrrx"},"updated":{"$t":"2016-07-28T13:41:55.267Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"10/18/2013"},"content":{"type":"text","$t":"speaker: No talk, affiliation: Fall Break"},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od6/public/values/clrrx"}],"gsx$date":{"$t":"10/18/2013"},"gsx$speaker":{"$t":"No talk"},"gsx$affiliation":{"$t":"Fall Break"},"gsx$title":{"$t":""},"gsx$abstract":{"$t":""},"gsx$video":{"$t":""},"gsx$videonotes":{"$t":""},"gsx$slides":{"$t":""},"gsx$slidesnotes":{"$t":""},"gsx$livestream":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od6/public/values/cyevm"},"updated":{"$t":"2016-07-28T13:41:55.267Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"10/25/2013"},"content":{"type":"text","$t":"speaker: Femi Alabi, affiliation: UNC-CH Computer Science, title: Comparative Visualization of Ensembles, abstract: By definition, an ensemble is a set of surfaces or volumes derived from\n a series of simulations or experiments. Sometimes the series is run with differing initial conditions for one parameter to determine parameter sensitivity. The understanding and identification of visual similarities and differences among the shapes of members of an ensemble is an acute and growing challenge for researchers across the physical sciences.  In order to analyse the geometries resulting from these simulations and the effect changing parameters exert on their formation, one must first decide how to present the ensemble to a form that is digestible by the researcher while simultaneously keeping the human in the loop in order for he/she to freely explore the effect parameter changes on shapes in their data.\n\u003cp\u003e\nThis talk will cover work that is being done at UNC-Chapel Hill on techniques for exploring, describing and summarizing generic shape trends in geometric ensemble space. A methodology for deciding common goals and tasks for exploring geometric ensembles will be presented as well as prior work on a novel single image view to facilitate simultaneous analysis of one such task: surface comparison., video: http://compsci.capture.duke.edu/Panopto/Pages/Viewer/Default.aspx?id\u003dd650aa0e-6610-4861-991f-f6796f4044bc, livestream: http://compsci.capture.duke.edu/Panopto/Pages/Viewer/Default.aspx?id\u003dd650aa0e-6610-4861-991f-f6796f4044bc"},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od6/public/values/cyevm"}],"gsx$date":{"$t":"10/25/2013"},"gsx$speaker":{"$t":"Femi Alabi"},"gsx$affiliation":{"$t":"UNC-CH Computer Science"},"gsx$title":{"$t":"Comparative Visualization of Ensembles"},"gsx$abstract":{"$t":"By definition, an ensemble is a set of surfaces or volumes derived from\n a series of simulations or experiments. Sometimes the series is run with differing initial conditions for one parameter to determine parameter sensitivity. The understanding and identification of visual similarities and differences among the shapes of members of an ensemble is an acute and growing challenge for researchers across the physical sciences.  In order to analyse the geometries resulting from these simulations and the effect changing parameters exert on their formation, one must first decide how to present the ensemble to a form that is digestible by the researcher while simultaneously keeping the human in the loop in order for he/she to freely explore the effect parameter changes on shapes in their data.\n\u003cp\u003e\nThis talk will cover work that is being done at UNC-Chapel Hill on techniques for exploring, describing and summarizing generic shape trends in geometric ensemble space. A methodology for deciding common goals and tasks for exploring geometric ensembles will be presented as well as prior work on a novel single image view to facilitate simultaneous analysis of one such task: surface comparison."},"gsx$video":{"$t":"http://compsci.capture.duke.edu/Panopto/Pages/Viewer/Default.aspx?id\u003dd650aa0e-6610-4861-991f-f6796f4044bc"},"gsx$videonotes":{"$t":""},"gsx$slides":{"$t":""},"gsx$slidesnotes":{"$t":""},"gsx$livestream":{"$t":"http://compsci.capture.duke.edu/Panopto/Pages/Viewer/Default.aspx?id\u003dd650aa0e-6610-4861-991f-f6796f4044bc"}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od6/public/values/cztg3"},"updated":{"$t":"2016-07-28T13:41:55.267Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"11/1/2013"},"content":{"type":"text","$t":"speaker: Katherine Franz \u0026 Stephen Craig, affiliation: Duke Chemistry, title: Creating Graphical Abstracts to Improve the Process of Scientific Research, abstract: One program. 10 students. 9 different labs. 3 departments. 8 weeks.\u003cbr\u003e\nLot of challenges for student researchers: Understand project, Communicate project, Collect data, Analyze data, Present results, Be coherent, Be interesting, Be trustworthy, Be brilliant.\n\u003cp\u003e\nHow do you get students who are new to campus and to each other invested quickly into a research project so that they can make meaningful progress during a short summer research stint? To attack this challenge with our cohort of students in the NSF-REU CASMM program (Chemistry and Applications of Smart Molecules \u0026 Materials), we had them create visual images of their research project at both the very beginning and the end of their stay.  In this presentation, we will discuss how we used 3-minute mini-presentations and graphical abstracts to train students in how to think about science, communicate science, and help themselves understand their own projects through the process of image creation.\n\u003cp\u003e\n(work done with Andrew Franks \u0026amp; Julia Roberts), video: http://compsci.capture.duke.edu/Panopto/Pages/Viewer/Default.aspx?id\u003d1694dc9d-f439-4c06-a5e5-56de7c979686, livestream: http://compsci.capture.duke.edu/Panopto/Pages/Viewer/Default.aspx?id\u003d1694dc9d-f439-4c06-a5e5-56de7c979686"},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od6/public/values/cztg3"}],"gsx$date":{"$t":"11/1/2013"},"gsx$speaker":{"$t":"Katherine Franz \u0026 Stephen Craig"},"gsx$affiliation":{"$t":"Duke Chemistry"},"gsx$title":{"$t":"Creating Graphical Abstracts to Improve the Process of Scientific Research"},"gsx$abstract":{"$t":"One program. 10 students. 9 different labs. 3 departments. 8 weeks.\u003cbr\u003e\nLot of challenges for student researchers: Understand project, Communicate project, Collect data, Analyze data, Present results, Be coherent, Be interesting, Be trustworthy, Be brilliant.\n\u003cp\u003e\nHow do you get students who are new to campus and to each other invested quickly into a research project so that they can make meaningful progress during a short summer research stint? To attack this challenge with our cohort of students in the NSF-REU CASMM program (Chemistry and Applications of Smart Molecules \u0026 Materials), we had them create visual images of their research project at both the very beginning and the end of their stay.  In this presentation, we will discuss how we used 3-minute mini-presentations and graphical abstracts to train students in how to think about science, communicate science, and help themselves understand their own projects through the process of image creation.\n\u003cp\u003e\n(work done with Andrew Franks \u0026amp; Julia Roberts)"},"gsx$video":{"$t":"http://compsci.capture.duke.edu/Panopto/Pages/Viewer/Default.aspx?id\u003d1694dc9d-f439-4c06-a5e5-56de7c979686"},"gsx$videonotes":{"$t":""},"gsx$slides":{"$t":""},"gsx$slidesnotes":{"$t":""},"gsx$livestream":{"$t":"http://compsci.capture.duke.edu/Panopto/Pages/Viewer/Default.aspx?id\u003d1694dc9d-f439-4c06-a5e5-56de7c979686"}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od6/public/values/d180g"},"updated":{"$t":"2016-07-28T13:41:55.267Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"11/8/2013"},"content":{"type":"text","$t":"speaker: Nick Jones, affiliation: Narrow Design, title: Hollywood's Visual Vapor, abstract: Sci-fi filmmakers play make-believe with impossible interfaces and fantastical data visualizations. Audiences eat them up. Then real-life designers spend the next decade trying to turn what we saw into reality...and how to make money doing it from home., video: http://compsci.capture.duke.edu/Panopto/Pages/Viewer/Default.aspx?id\u003dd69ee21c-caf3-4435-8c44-6d96ff74738f, livestream: http://compsci.capture.duke.edu/Panopto/Pages/Viewer/Default.aspx?id\u003dd69ee21c-caf3-4435-8c44-6d96ff74738f"},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od6/public/values/d180g"}],"gsx$date":{"$t":"11/8/2013"},"gsx$speaker":{"$t":"Nick Jones"},"gsx$affiliation":{"$t":"Narrow Design"},"gsx$title":{"$t":"Hollywood's Visual Vapor"},"gsx$abstract":{"$t":"Sci-fi filmmakers play make-believe with impossible interfaces and fantastical data visualizations. Audiences eat them up. Then real-life designers spend the next decade trying to turn what we saw into reality...and how to make money doing it from home."},"gsx$video":{"$t":"http://compsci.capture.duke.edu/Panopto/Pages/Viewer/Default.aspx?id\u003dd69ee21c-caf3-4435-8c44-6d96ff74738f"},"gsx$videonotes":{"$t":""},"gsx$slides":{"$t":""},"gsx$slidesnotes":{"$t":""},"gsx$livestream":{"$t":"http://compsci.capture.duke.edu/Panopto/Pages/Viewer/Default.aspx?id\u003dd69ee21c-caf3-4435-8c44-6d96ff74738f"}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od6/public/values/d2mkx"},"updated":{"$t":"2016-07-28T13:41:55.267Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"11/15/2013"},"content":{"type":"text","$t":"speaker: David Hill, affiliation: NCSU College of Design, title: Almost-Always Rules of Graphic Presentations: effective techniques for researchers and designers, abstract: Researchers and designers face a similar communication challenge: how to illustrate complex ideas in clear and compelling graphic presentations. This lecture will outline a series of guiding principles for crafting graphic presentations in a \"what-to-do, what-not-to-do\" format. Examples will demonstrate strategies for font selection, composition, color, formatting, and integrating text and imagery into an effective visual narrative. These almost-always rules provide a basic framework for organizing presentations, but as the name implies, there are times when rules are meant to be broken.\n\u003cp\u003e\nThis presentation is adapted from a lecture in the ARC 450 Digital Representation course in the NCSU College of Design, School of Architecture., video: http://compsci.capture.duke.edu/Panopto/Pages/Viewer/Default.aspx?id\u003d01bb6608-9013-469f-b64c-bc5411869013, livestream: http://compsci.capture.duke.edu/Panopto/Pages/Viewer/Default.aspx?id\u003d01bb6608-9013-469f-b64c-bc5411869013"},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od6/public/values/d2mkx"}],"gsx$date":{"$t":"11/15/2013"},"gsx$speaker":{"$t":"David Hill"},"gsx$affiliation":{"$t":"NCSU College of Design"},"gsx$title":{"$t":"Almost-Always Rules of Graphic Presentations: effective techniques for researchers and designers"},"gsx$abstract":{"$t":"Researchers and designers face a similar communication challenge: how to illustrate complex ideas in clear and compelling graphic presentations. This lecture will outline a series of guiding principles for crafting graphic presentations in a \"what-to-do, what-not-to-do\" format. Examples will demonstrate strategies for font selection, composition, color, formatting, and integrating text and imagery into an effective visual narrative. These almost-always rules provide a basic framework for organizing presentations, but as the name implies, there are times when rules are meant to be broken.\n\u003cp\u003e\nThis presentation is adapted from a lecture in the ARC 450 Digital Representation course in the NCSU College of Design, School of Architecture."},"gsx$video":{"$t":"http://compsci.capture.duke.edu/Panopto/Pages/Viewer/Default.aspx?id\u003d01bb6608-9013-469f-b64c-bc5411869013"},"gsx$videonotes":{"$t":""},"gsx$slides":{"$t":""},"gsx$slidesnotes":{"$t":""},"gsx$livestream":{"$t":"http://compsci.capture.duke.edu/Panopto/Pages/Viewer/Default.aspx?id\u003d01bb6608-9013-469f-b64c-bc5411869013"}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od6/public/values/cssly"},"updated":{"$t":"2016-07-28T13:41:55.267Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"11/22/2013"},"content":{"type":"text","$t":"speaker: Alexandros Iliopoulos, affiliation: Duke Computer Science, title: Big Snapshot Stitching with Scarce Overlap, abstract: We address certain properties that arise in gigapixel-scale image stitching for snapshot images captured with a novel micro-camera array system, AWARE-2. This system features a greatly extended field of view and high optical resolution, offering unique sensing capabilities for a host of important applications. However, three simultaneously arising conditions pose a challenge to existing approaches to image stitching, with regard to the quality of the output image as well as the automation and efficiency of the image composition process. Put simply, they may be described as the sparse, geometrically irregular, and noisy (S.I.N.) overlap amongst the fields of view of the constituent micro-cameras. We introduce a computational pipeline for image stitching under these conditions, which is scalable in terms of complexity and efficiency. With it, we also substantially reduce or eliminate ghosting effects due to misalignment factors, without entailing manual intervention. Our present implementation of the pipeline leverages the combined use of multicore and GPU architectures. We present experimental results with the pipeline on real image data acquired with AWARE-2., video: http://compsci.capture.duke.edu/Panopto/Pages/Viewer/Default.aspx?id\u003d46d362ab-bdb4-4eb5-98ba-9ef5f7b9c3df, livestream: http://compsci.capture.duke.edu/Panopto/Pages/Viewer/Default.aspx?id\u003d46d362ab-bdb4-4eb5-98ba-9ef5f7b9c3df"},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od6/public/values/cssly"}],"gsx$date":{"$t":"11/22/2013"},"gsx$speaker":{"$t":"Alexandros Iliopoulos"},"gsx$affiliation":{"$t":"Duke Computer Science"},"gsx$title":{"$t":"Big Snapshot Stitching with Scarce Overlap"},"gsx$abstract":{"$t":"We address certain properties that arise in gigapixel-scale image stitching for snapshot images captured with a novel micro-camera array system, AWARE-2. This system features a greatly extended field of view and high optical resolution, offering unique sensing capabilities for a host of important applications. However, three simultaneously arising conditions pose a challenge to existing approaches to image stitching, with regard to the quality of the output image as well as the automation and efficiency of the image composition process. Put simply, they may be described as the sparse, geometrically irregular, and noisy (S.I.N.) overlap amongst the fields of view of the constituent micro-cameras. We introduce a computational pipeline for image stitching under these conditions, which is scalable in terms of complexity and efficiency. With it, we also substantially reduce or eliminate ghosting effects due to misalignment factors, without entailing manual intervention. Our present implementation of the pipeline leverages the combined use of multicore and GPU architectures. We present experimental results with the pipeline on real image data acquired with AWARE-2."},"gsx$video":{"$t":"http://compsci.capture.duke.edu/Panopto/Pages/Viewer/Default.aspx?id\u003d46d362ab-bdb4-4eb5-98ba-9ef5f7b9c3df"},"gsx$videonotes":{"$t":""},"gsx$slides":{"$t":""},"gsx$slidesnotes":{"$t":""},"gsx$livestream":{"$t":"http://compsci.capture.duke.edu/Panopto/Pages/Viewer/Default.aspx?id\u003d46d362ab-bdb4-4eb5-98ba-9ef5f7b9c3df"}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od6/public/values/cu76f"},"updated":{"$t":"2016-07-28T13:41:55.267Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"11/29/2013"},"content":{"type":"text","$t":"speaker: No talk, affiliation: Thanksgiving"},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od6/public/values/cu76f"}],"gsx$date":{"$t":"11/29/2013"},"gsx$speaker":{"$t":"No talk"},"gsx$affiliation":{"$t":"Thanksgiving"},"gsx$title":{"$t":""},"gsx$abstract":{"$t":""},"gsx$video":{"$t":""},"gsx$videonotes":{"$t":""},"gsx$slides":{"$t":""},"gsx$slidesnotes":{"$t":""},"gsx$livestream":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od6/public/values/cvlqs"},"updated":{"$t":"2016-07-28T13:41:55.267Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"12/6/2013"},"content":{"type":"text","$t":"speaker: Eric Ragan, affiliation: Oak Ridge National Lab, title: A Human-Centered Approach to Studying the Spatial Visualization of Non-Spatial Information, abstract: Many visual applications, such as visual analytics tools and educational games, employ spatial information presentations to support data exploration and improve understanding. However, it is not well understood how to take advantage of spatial information layouts, especially when dealing with large data sets, abstract information, and multiple display options. As a result, it is often unclear how to effectively design spatial visualizations for learning and sense-making. My research addresses this problem through controlled experimentation and observation. My work focuses on the evaluation of interface design factors for information presentations on physically-large 2D displays and in immersive 3D virtual reality systems. In this talk, I will discuss several projects that evaluate task performance and information processing strategies, with a specific example involving scientific data exploration. Overall, the results suggest that supplemental spatial information can affect mental strategies and support performance improvements for cognitive processing, but the effectiveness of spatial presentations is dependent on the nature of the task and a meaningful use of space. I will close with a discussion of how the lessons learned from user studies affect the design of visual analytics tools., video: http://compsci.capture.duke.edu/Panopto/Pages/Viewer/Default.aspx?id\u003db6ec972f-bc52-4f92-944b-2e9c30c60022, livestream: http://compsci.capture.duke.edu/Panopto/Pages/Viewer/Default.aspx?id\u003db6ec972f-bc52-4f92-944b-2e9c30c60022"},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od6/public/values/cvlqs"}],"gsx$date":{"$t":"12/6/2013"},"gsx$speaker":{"$t":"Eric Ragan"},"gsx$affiliation":{"$t":"Oak Ridge National Lab"},"gsx$title":{"$t":"A Human-Centered Approach to Studying the Spatial Visualization of Non-Spatial Information"},"gsx$abstract":{"$t":"Many visual applications, such as visual analytics tools and educational games, employ spatial information presentations to support data exploration and improve understanding. However, it is not well understood how to take advantage of spatial information layouts, especially when dealing with large data sets, abstract information, and multiple display options. As a result, it is often unclear how to effectively design spatial visualizations for learning and sense-making. My research addresses this problem through controlled experimentation and observation. My work focuses on the evaluation of interface design factors for information presentations on physically-large 2D displays and in immersive 3D virtual reality systems. In this talk, I will discuss several projects that evaluate task performance and information processing strategies, with a specific example involving scientific data exploration. Overall, the results suggest that supplemental spatial information can affect mental strategies and support performance improvements for cognitive processing, but the effectiveness of spatial presentations is dependent on the nature of the task and a meaningful use of space. I will close with a discussion of how the lessons learned from user studies affect the design of visual analytics tools."},"gsx$video":{"$t":"http://compsci.capture.duke.edu/Panopto/Pages/Viewer/Default.aspx?id\u003db6ec972f-bc52-4f92-944b-2e9c30c60022"},"gsx$videonotes":{"$t":""},"gsx$slides":{"$t":""},"gsx$slidesnotes":{"$t":""},"gsx$livestream":{"$t":"http://compsci.capture.duke.edu/Panopto/Pages/Viewer/Default.aspx?id\u003db6ec972f-bc52-4f92-944b-2e9c30c60022"}}]}});
// API callback
Tabletop.singleton.loadSheet({"version":"1.0","encoding":"UTF-8","feed":{"xmlns":"http://www.w3.org/2005/Atom","xmlns$openSearch":"http://a9.com/-/spec/opensearchrss/1.0/","xmlns$gsx":"http://schemas.google.com/spreadsheets/2006/extended","id":{"$t":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od1/public/values"},"updated":{"$t":"2015-12-11T12:54:10.655Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"Fall_2008"},"link":[{"rel":"alternate","type":"application/atom+xml","href":"https://docs.google.com/spreadsheets/d/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/pubhtml"},{"rel":"http://schemas.google.com/g/2005#feed","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od1/public/values"},{"rel":"http://schemas.google.com/g/2005#post","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od1/public/values"},{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od1/public/values?alt\u003djson-in-script"}],"author":[{"name":{"$t":"eric.e.monson"},"email":{"$t":"eric.e.monson@gmail.com"}}],"openSearch$totalResults":{"$t":"13"},"openSearch$startIndex":{"$t":"1"},"entry":[{"id":{"$t":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od1/public/values/cokwr"},"updated":{"$t":"2015-12-11T12:54:10.655Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"9/5/2008"},"content":{"type":"text","$t":"speaker: Rachael Brady, affiliation: Visualization Technology Group, title: Introducing the Friday Forum, abstract: I like to start each semester off with an overview/welcome talk. Today I will discuss some projects that happened through the VTG group over the summer, such as the LINK Media Wall (Steve Feller), an Avatar Creation Ensemble (Wayne Godwin/Oriana Wen), a DiVE experience for teaching Chemistry (Shelly Bloom/Marcel Yang), and soundSpace (Scott Lindroth/Steve Feller) an Installation at the Museum of Life and Science."},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od1/public/values/cokwr"}],"gsx$date":{"$t":"9/5/2008"},"gsx$speaker":{"$t":"Rachael Brady"},"gsx$affiliation":{"$t":"Visualization Technology Group"},"gsx$title":{"$t":"Introducing the Friday Forum"},"gsx$abstract":{"$t":"I like to start each semester off with an overview/welcome talk. Today I will discuss some projects that happened through the VTG group over the summer, such as the LINK Media Wall (Steve Feller), an Avatar Creation Ensemble (Wayne Godwin/Oriana Wen), a DiVE experience for teaching Chemistry (Shelly Bloom/Marcel Yang), and soundSpace (Scott Lindroth/Steve Feller) an Installation at the Museum of Life and Science."},"gsx$video":{"$t":""},"gsx$videonotes":{"$t":""},"gsx$slides":{"$t":""},"gsx$slidesnotes":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od1/public/values/cpzh4"},"updated":{"$t":"2015-12-11T12:54:10.655Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"9/12/2008"},"content":{"type":"text","$t":"speaker: Matt Reynolds, affiliation: ECE, title: Smart Surfaces and Smart Spaces for Smart Homes, abstract: With the recent emergence of affordable, large, high resolution displays for the home, we can now easily imagine table scale and wall scale displays that interact with room occupants in a variety of different application contexts beyond simple movie and photo playback. The \"Microsoft Surface\" interactive table is perhaps the best publicized recent example of an interactive surface appropriate for home use, but there are a variety of interesting approaches for table scale and room scale interaction beyond the camera based approaches used by Microsoft. I will present some examples from my own work and that of colleagues showing a variety of sensing technologies that enable large interactive displays, as well as some of the novel interfaces that have been developed on top of those platforms."},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od1/public/values/cpzh4"}],"gsx$date":{"$t":"9/12/2008"},"gsx$speaker":{"$t":"Matt Reynolds"},"gsx$affiliation":{"$t":"ECE"},"gsx$title":{"$t":"Smart Surfaces and Smart Spaces for Smart Homes"},"gsx$abstract":{"$t":"With the recent emergence of affordable, large, high resolution displays for the home, we can now easily imagine table scale and wall scale displays that interact with room occupants in a variety of different application contexts beyond simple movie and photo playback. The \"Microsoft Surface\" interactive table is perhaps the best publicized recent example of an interactive surface appropriate for home use, but there are a variety of interesting approaches for table scale and room scale interaction beyond the camera based approaches used by Microsoft. I will present some examples from my own work and that of colleagues showing a variety of sensing technologies that enable large interactive displays, as well as some of the novel interfaces that have been developed on top of those platforms."},"gsx$video":{"$t":""},"gsx$videonotes":{"$t":""},"gsx$slides":{"$t":""},"gsx$slidesnotes":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od1/public/values/cre1l"},"updated":{"$t":"2015-12-11T12:54:10.655Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"9/19/2008"},"content":{"type":"text","$t":"speaker: Shawn Miller, affiliation: Center for Instructional Technology, title: Everybody's doing it: web-based visualizations and mashups in the social sciences, abstract: Last year, the Center for Instructional Technology provided grants for faculty interested in more deeply pursuing the addition of visualizations to their teaching. Many of the projects CIT funded also had a strong impact on the research faculty have been doing. Web2.0 visualization tools and sites (Google Earth/Maps, Many Eyes, etc) have made the creation of data-driven visualizations more readily available, less complicated, and more easily embedded into public websites, in turn making them more attractive to researchers with only a casual or passive interest in visualization. In this talk, I'll show examples of a few of the technologies (such as Google Earth/Maps and MIT's SIMILE project) used in recent social science projects at Duke and discuss both the positive impact and pitfalls of using them."},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od1/public/values/cre1l"}],"gsx$date":{"$t":"9/19/2008"},"gsx$speaker":{"$t":"Shawn Miller"},"gsx$affiliation":{"$t":"Center for Instructional Technology"},"gsx$title":{"$t":"Everybody's doing it: web-based visualizations and mashups in the social sciences"},"gsx$abstract":{"$t":"Last year, the Center for Instructional Technology provided grants for faculty interested in more deeply pursuing the addition of visualizations to their teaching. Many of the projects CIT funded also had a strong impact on the research faculty have been doing. Web2.0 visualization tools and sites (Google Earth/Maps, Many Eyes, etc) have made the creation of data-driven visualizations more readily available, less complicated, and more easily embedded into public websites, in turn making them more attractive to researchers with only a casual or passive interest in visualization. In this talk, I'll show examples of a few of the technologies (such as Google Earth/Maps and MIT's SIMILE project) used in recent social science projects at Duke and discuss both the positive impact and pitfalls of using them."},"gsx$video":{"$t":""},"gsx$videonotes":{"$t":""},"gsx$slides":{"$t":""},"gsx$slidesnotes":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od1/public/values/chk2m"},"updated":{"$t":"2015-12-11T12:54:10.655Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"9/26/2008"},"content":{"type":"text","$t":"speaker: Jennifer Swenson, affiliation: Nicholas School of the Environment, title: Monitoring Spring from Satellites, abstract: The green flush of spring is readily observable from space. Near daily satellites provide wall-to-wall images of the variable patterns of spring green-up over space and time. Once patterns of spring onset and duration are characterized, we can begin to examine environmental variables that influence spring onset for a given year and a particular place (e.g. days since last freeze, soil temperature, topography). This NASA-funded project examines spring patterns in North Carolina in the context of climate change's potential influence on earlier spring onset dates and an extended growing season, potentially altering ecosystem productivity."},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od1/public/values/chk2m"}],"gsx$date":{"$t":"9/26/2008"},"gsx$speaker":{"$t":"Jennifer Swenson"},"gsx$affiliation":{"$t":"Nicholas School of the Environment"},"gsx$title":{"$t":"Monitoring Spring from Satellites"},"gsx$abstract":{"$t":"The green flush of spring is readily observable from space. Near daily satellites provide wall-to-wall images of the variable patterns of spring green-up over space and time. Once patterns of spring onset and duration are characterized, we can begin to examine environmental variables that influence spring onset for a given year and a particular place (e.g. days since last freeze, soil temperature, topography). This NASA-funded project examines spring patterns in North Carolina in the context of climate change's potential influence on earlier spring onset dates and an extended growing season, potentially altering ecosystem productivity."},"gsx$video":{"$t":""},"gsx$videonotes":{"$t":""},"gsx$slides":{"$t":""},"gsx$slidesnotes":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od1/public/values/ciyn3"},"updated":{"$t":"2015-12-11T12:54:10.655Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"10/3/2008"},"content":{"type":"text","$t":"speaker: Bill Seaman, affiliation: Art, Art History, Visual Studies, title: Transdisciplinary Research-Building Bridging Languages, abstract: We have often spoken of interdisciplinary research in the past. In transdisciplinary research, a series of disciplines are bridged. Because no singular discipline or history of that discipline can be used to articulate the emergent work that is arising, the term transdisciplinary is used, suggesting that such research goes beyond any individual discipline or coupling of disciplines. Transdisciplinary research brings a set of disciplines together in the service of new forms of poetic expression and emergent knowledge production. Often such research involves new approaches to technology, interactivity and computational paradigms. I have been working on a number of collaborations since 1980. This includes projects involving Scientists,Computer Programmers, Designers, Artists, Dancers, Choreographers, Musicians, Media Theorists, Architects, and Students. I will discuss the research that I am exploring which involves transdisciplinary collaboration and I will show relevant materials to outline some of these projects-some completed and some still in progress. I will also discuss the potentially valuable nature of projects with very long-term research goals."},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od1/public/values/ciyn3"}],"gsx$date":{"$t":"10/3/2008"},"gsx$speaker":{"$t":"Bill Seaman"},"gsx$affiliation":{"$t":"Art, Art History, Visual Studies"},"gsx$title":{"$t":"Transdisciplinary Research-Building Bridging Languages"},"gsx$abstract":{"$t":"We have often spoken of interdisciplinary research in the past. In transdisciplinary research, a series of disciplines are bridged. Because no singular discipline or history of that discipline can be used to articulate the emergent work that is arising, the term transdisciplinary is used, suggesting that such research goes beyond any individual discipline or coupling of disciplines. Transdisciplinary research brings a set of disciplines together in the service of new forms of poetic expression and emergent knowledge production. Often such research involves new approaches to technology, interactivity and computational paradigms. I have been working on a number of collaborations since 1980. This includes projects involving Scientists,Computer Programmers, Designers, Artists, Dancers, Choreographers, Musicians, Media Theorists, Architects, and Students. I will discuss the research that I am exploring which involves transdisciplinary collaboration and I will show relevant materials to outline some of these projects-some completed and some still in progress. I will also discuss the potentially valuable nature of projects with very long-term research goals."},"gsx$video":{"$t":""},"gsx$videonotes":{"$t":""},"gsx$slides":{"$t":""},"gsx$slidesnotes":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od1/public/values/ckd7g"},"updated":{"$t":"2015-12-11T12:54:10.655Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"10/10/2008"},"content":{"type":"text","$t":"speaker: Xunlei Wu, affiliation: Duke RENCI Center, title: Introducing the Multitouch Wall, abstract: The new Duke-RENCI Center on the first floor of the OIT-Telecommunications Building features a 13-foot X 5-foot Multi-Touch Visualization Wall that was developed in-house by a team of RENCI staff scientists. This forum will focus on the development of high-resolution touch-sensitive displays, and the role that gesture-based interaction can play in research and discovery."},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od1/public/values/ckd7g"}],"gsx$date":{"$t":"10/10/2008"},"gsx$speaker":{"$t":"Xunlei Wu"},"gsx$affiliation":{"$t":"Duke RENCI Center"},"gsx$title":{"$t":"Introducing the Multitouch Wall"},"gsx$abstract":{"$t":"The new Duke-RENCI Center on the first floor of the OIT-Telecommunications Building features a 13-foot X 5-foot Multi-Touch Visualization Wall that was developed in-house by a team of RENCI staff scientists. This forum will focus on the development of high-resolution touch-sensitive displays, and the role that gesture-based interaction can play in research and discovery."},"gsx$video":{"$t":""},"gsx$videonotes":{"$t":""},"gsx$slides":{"$t":""},"gsx$slidesnotes":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od1/public/values/clrrx"},"updated":{"$t":"2015-12-11T12:54:10.655Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"10/17/2008"},"content":{"type":"text","$t":"speaker: Andrea Stevenson-Won, affiliation: Biomodal LLC and Visualization Technology Group, title: Forensic Facial Reconstruction- Adding detail, keeping it honest, abstract: Reconstructing the face of an unidentified person from their skull is a staple of museum display and crime lab dramatizations. But the more \"realistic\" a portrait based solely on the skull becomes, the further away it drifts from being a accurate representation of the individual in life. A useful reconstruction for identification purposes is a less detailed representation with purposeful ambiguity. How can museums and other institutions use new techniques in visualization to create detailed reconstructions in their displays without misleading?"},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od1/public/values/clrrx"}],"gsx$date":{"$t":"10/17/2008"},"gsx$speaker":{"$t":"Andrea Stevenson-Won"},"gsx$affiliation":{"$t":"Biomodal LLC and Visualization Technology Group"},"gsx$title":{"$t":"Forensic Facial Reconstruction- Adding detail, keeping it honest"},"gsx$abstract":{"$t":"Reconstructing the face of an unidentified person from their skull is a staple of museum display and crime lab dramatizations. But the more \"realistic\" a portrait based solely on the skull becomes, the further away it drifts from being a accurate representation of the individual in life. A useful reconstruction for identification purposes is a less detailed representation with purposeful ambiguity. How can museums and other institutions use new techniques in visualization to create detailed reconstructions in their displays without misleading?"},"gsx$video":{"$t":""},"gsx$videonotes":{"$t":""},"gsx$slides":{"$t":""},"gsx$slidesnotes":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od1/public/values/cyevm"},"updated":{"$t":"2015-12-11T12:54:10.655Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"10/24/2008"},"content":{"type":"text","$t":"speaker: Ashwin Wagadarikar, affiliation: ECE, title: Spectral imaging using a coded aperture snapshot spectral imager, abstract: Spectral imaging is a method to capture a three dimensional datacube of information where the two dimensional spatial image obtained from a regular camera is complemented with spectral information at every pixel. Unlike conventional spectral imagers that require temporal scanning to capture a datacube, our novel coded aperture snapshot spectral imagers (CASSI) are able to recover a spectral image from just one snapshot, two dimensional, coded projection of the three dimensional datacube. We will present the design and reconstruction algorithm for a single disperser-CASSI prototype, before demonstrating its application to video rate spectral imaging of a fast changing scene."},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od1/public/values/cyevm"}],"gsx$date":{"$t":"10/24/2008"},"gsx$speaker":{"$t":"Ashwin Wagadarikar"},"gsx$affiliation":{"$t":"ECE"},"gsx$title":{"$t":"Spectral imaging using a coded aperture snapshot spectral imager"},"gsx$abstract":{"$t":"Spectral imaging is a method to capture a three dimensional datacube of information where the two dimensional spatial image obtained from a regular camera is complemented with spectral information at every pixel. Unlike conventional spectral imagers that require temporal scanning to capture a datacube, our novel coded aperture snapshot spectral imagers (CASSI) are able to recover a spectral image from just one snapshot, two dimensional, coded projection of the three dimensional datacube. We will present the design and reconstruction algorithm for a single disperser-CASSI prototype, before demonstrating its application to video rate spectral imaging of a fast changing scene."},"gsx$video":{"$t":""},"gsx$videonotes":{"$t":""},"gsx$slides":{"$t":""},"gsx$slidesnotes":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od1/public/values/cztg3"},"updated":{"$t":"2015-12-11T12:54:10.655Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"10/31/2008"},"content":{"type":"text","$t":"speaker: Stuart McKinnon, affiliation: Ophthalmology, title: Dynamic Optic Nerve Compliance Measurement Using SD-OCT, abstract: The biomechanical properties of the optic nerve head may play an important role in the development of glaucoma. Using spectral domain ocular coherence tomography (SD-OCT), we can obtain volume scans of the optic nerve head at various applied pressures. Displacement maps of the optic nerve head surface are then created using ImageJ and Amira. Showing a quantitative difference in compliance between normal and glaucoma patients may provide a novel tool for earlier diagnosis of glaucoma."},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od1/public/values/cztg3"}],"gsx$date":{"$t":"10/31/2008"},"gsx$speaker":{"$t":"Stuart McKinnon"},"gsx$affiliation":{"$t":"Ophthalmology"},"gsx$title":{"$t":"Dynamic Optic Nerve Compliance Measurement Using SD-OCT"},"gsx$abstract":{"$t":"The biomechanical properties of the optic nerve head may play an important role in the development of glaucoma. Using spectral domain ocular coherence tomography (SD-OCT), we can obtain volume scans of the optic nerve head at various applied pressures. Displacement maps of the optic nerve head surface are then created using ImageJ and Amira. Showing a quantitative difference in compliance between normal and glaucoma patients may provide a novel tool for earlier diagnosis of glaucoma."},"gsx$video":{"$t":""},"gsx$videonotes":{"$t":""},"gsx$slides":{"$t":""},"gsx$slidesnotes":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od1/public/values/d180g"},"updated":{"$t":"2015-12-11T12:54:10.655Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"11/7/2008"},"content":{"type":"text","$t":"speaker: Taichiro Sakagami, affiliation: Nicholas School of the Environment, title: Visualizing El Nino: Using real data, but simplifying to communicate the essence., abstract: El Nino is one example of a kind of climate variability that is well understood by a small group of scientists, but very poorly understood by educated citizens. I believe visualization technology is one method that could help dissseminate current scientific knowlege about El Nino to the educated public. Over the last year I have created an animation that's ment to communicate the essence of El Nino phenomenon. This talk will cover my experience in this process and illustrate the steps of creating the animation. Of course, I will also show the current version of my El Nino animation."},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od1/public/values/d180g"}],"gsx$date":{"$t":"11/7/2008"},"gsx$speaker":{"$t":"Taichiro Sakagami"},"gsx$affiliation":{"$t":"Nicholas School of the Environment"},"gsx$title":{"$t":"Visualizing El Nino: Using real data, but simplifying to communicate the essence."},"gsx$abstract":{"$t":"El Nino is one example of a kind of climate variability that is well understood by a small group of scientists, but very poorly understood by educated citizens. I believe visualization technology is one method that could help dissseminate current scientific knowlege about El Nino to the educated public. Over the last year I have created an animation that's ment to communicate the essence of El Nino phenomenon. This talk will cover my experience in this process and illustrate the steps of creating the animation. Of course, I will also show the current version of my El Nino animation."},"gsx$video":{"$t":""},"gsx$videonotes":{"$t":""},"gsx$slides":{"$t":""},"gsx$slidesnotes":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od1/public/values/d2mkx"},"updated":{"$t":"2015-12-11T12:54:10.655Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"11/14/2008"},"content":{"type":"text","$t":"speaker: Eric Monson, affiliation: Visualization Technology Group, title: Highlights from the 2008 Vis Conference, abstract: The IEEE Visualization conference is a weeklong event which includes Scientific Visualization, InfoVis and Visual Analytics. Instead of trying to give you an overview of what was presented there last month, I will focus on a couple topics I found particularly interesting. First, I will cover the large group of new classes which researchers at Sandia National Labs have recently added to VTK (the Visualization Toolkit) to handle InfoVis, including graphs (networks), graph algorithms, new data structures and geovis. Next, I will briefly demonstrate an InfoVis tool from IBM research, Word Tree, which is used to visualize and explore text, somewhat like a tag cloud, while preserving the context of the words in the document. Finally, if there is time, I will show a couple interesting entries from the InfoVis Art Show."},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od1/public/values/d2mkx"}],"gsx$date":{"$t":"11/14/2008"},"gsx$speaker":{"$t":"Eric Monson"},"gsx$affiliation":{"$t":"Visualization Technology Group"},"gsx$title":{"$t":"Highlights from the 2008 Vis Conference"},"gsx$abstract":{"$t":"The IEEE Visualization conference is a weeklong event which includes Scientific Visualization, InfoVis and Visual Analytics. Instead of trying to give you an overview of what was presented there last month, I will focus on a couple topics I found particularly interesting. First, I will cover the large group of new classes which researchers at Sandia National Labs have recently added to VTK (the Visualization Toolkit) to handle InfoVis, including graphs (networks), graph algorithms, new data structures and geovis. Next, I will briefly demonstrate an InfoVis tool from IBM research, Word Tree, which is used to visualize and explore text, somewhat like a tag cloud, while preserving the context of the words in the document. Finally, if there is time, I will show a couple interesting entries from the InfoVis Art Show."},"gsx$video":{"$t":""},"gsx$videonotes":{"$t":""},"gsx$slides":{"$t":""},"gsx$slidesnotes":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od1/public/values/cssly"},"updated":{"$t":"2015-12-11T12:54:10.655Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"11/21/2008"},"content":{"type":"text","$t":"speaker: Ben Best, affiliation: Nicholas School of the Environment, title: Mapping and Visualization for Marine Biogeography, a Community Approach, abstract: Our ocean's biodiversity is disappearing before even being discovered and described. The Census of Marine Life (coml.org), a 10-year scientific initiative with a global network of scientists in more than 80 nations, are assessing the diversity, distribution, and abundance of life in the oceans, for final release in 2010. As part of this massive synthesis effort, a team based at Duke (comlmaps.org) is developing mapping and visualization tools and capacity across the Census projects for the purposes of scientific discovery, decision management, education and outreach. This talk will show highlights from a recent workshop we hosted. Visually intuitive and reproducible scientific workflows will enable researchers to easily fetch biological and environmental data, construct phylogenies mapped in space, model species richness and habitat, and project into past and future climates. Geotagged multimedia content will be aggregated across projects using RSS feeds for touring with Google Maps and Google Earth. Community-based spatial feedback on species ranges, model predictions, and management areas could be updated through the browser with internet mapping technologies. Potential interplay with related Duke technology from NESCent, RENCI and the DiVE will be mentioned, collaborations welcome. This is joint work with Patrick Halpin, El Fujioka, and Ben Donnelly"},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od1/public/values/cssly"}],"gsx$date":{"$t":"11/21/2008"},"gsx$speaker":{"$t":"Ben Best"},"gsx$affiliation":{"$t":"Nicholas School of the Environment"},"gsx$title":{"$t":"Mapping and Visualization for Marine Biogeography, a Community Approach"},"gsx$abstract":{"$t":"Our ocean's biodiversity is disappearing before even being discovered and described. The Census of Marine Life (coml.org), a 10-year scientific initiative with a global network of scientists in more than 80 nations, are assessing the diversity, distribution, and abundance of life in the oceans, for final release in 2010. As part of this massive synthesis effort, a team based at Duke (comlmaps.org) is developing mapping and visualization tools and capacity across the Census projects for the purposes of scientific discovery, decision management, education and outreach. This talk will show highlights from a recent workshop we hosted. Visually intuitive and reproducible scientific workflows will enable researchers to easily fetch biological and environmental data, construct phylogenies mapped in space, model species richness and habitat, and project into past and future climates. Geotagged multimedia content will be aggregated across projects using RSS feeds for touring with Google Maps and Google Earth. Community-based spatial feedback on species ranges, model predictions, and management areas could be updated through the browser with internet mapping technologies. Potential interplay with related Duke technology from NESCent, RENCI and the DiVE will be mentioned, collaborations welcome. This is joint work with Patrick Halpin, El Fujioka, and Ben Donnelly"},"gsx$video":{"$t":""},"gsx$videonotes":{"$t":""},"gsx$slides":{"$t":""},"gsx$slidesnotes":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od1/public/values/cu76f"},"updated":{"$t":"2015-12-11T12:54:10.655Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"12/5/2008"},"content":{"type":"text","$t":"speaker: Tim Lenoir, affiliation: ISIS, title: Virtual Peace, abstract: The Humanitarian Assistance Training Simulator brings together digital learning technologies and international humanitarian assistance efforts. Students and educators enter an immersive, multi-sensory game-based environment that simulates actual disaster relief and conflict resolution conditions in order to learn first-hand the necessary tools for sensitive and timely crisis response. We will demo the game-based learning environment and discuss the role of Duke CS students and faculty in building it."},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/od1/public/values/cu76f"}],"gsx$date":{"$t":"12/5/2008"},"gsx$speaker":{"$t":"Tim Lenoir"},"gsx$affiliation":{"$t":"ISIS"},"gsx$title":{"$t":"Virtual Peace"},"gsx$abstract":{"$t":"The Humanitarian Assistance Training Simulator brings together digital learning technologies and international humanitarian assistance efforts. Students and educators enter an immersive, multi-sensory game-based environment that simulates actual disaster relief and conflict resolution conditions in order to learn first-hand the necessary tools for sensitive and timely crisis response. We will demo the game-based learning environment and discuss the role of Duke CS students and faculty in building it."},"gsx$video":{"$t":""},"gsx$videonotes":{"$t":""},"gsx$slides":{"$t":""},"gsx$slidesnotes":{"$t":""}}]}});
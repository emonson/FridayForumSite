// API callback
Tabletop.singleton.loadSheet({"version":"1.0","encoding":"UTF-8","feed":{"xmlns":"http://www.w3.org/2005/Atom","xmlns$openSearch":"http://a9.com/-/spec/opensearchrss/1.0/","xmlns$gsx":"http://schemas.google.com/spreadsheets/2006/extended","id":{"$t":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/oco/public/values"},"updated":{"$t":"2015-12-11T12:54:10.655Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"Spring_2007"},"link":[{"rel":"alternate","type":"application/atom+xml","href":"https://docs.google.com/spreadsheets/d/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/pubhtml"},{"rel":"http://schemas.google.com/g/2005#feed","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/oco/public/values"},{"rel":"http://schemas.google.com/g/2005#post","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/oco/public/values"},{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/oco/public/values?alt\u003djson-in-script"}],"author":[{"name":{"$t":"eric.e.monson"},"email":{"$t":"eric.e.monson@gmail.com"}}],"openSearch$totalResults":{"$t":"13"},"openSearch$startIndex":{"$t":"1"},"entry":[{"id":{"$t":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/oco/public/values/cokwr"},"updated":{"$t":"2015-12-11T12:54:10.655Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"1/19/2007"},"content":{"type":"text","$t":"speaker: Melanie Wright, affiliation: Human Simulation and Patient Safety Center, title: Eye tracking in a human patient simulation environment: Data collection, coding, visualization, and analysis, abstract: One method of gaining insight into the underlying perceptual and cognitive performance in dynamic work environments is through the use of eye tracking measures. Eye tracking equipment can monitor the specific eye movements of practitioners. This information can be used to determine patterns of equipment use and information access. Researchers have used eye tracking techniques in a wide variety of environments for purposes such as comparing the scan patterns of experts and novices, evaluating menu scan behavior, or evaluating driver distraction. We are currently conducting a study evaluating various methods of assessing the differences between expert and novice performance in anesthesia. Among other performance measures, we are collecting eye tracking data while anesthesia providers treat two simulated anesthesia patients. The goal is to identify specific determinants of expertise in anesthesia. This information is expected to be useful for anesthesia training, assessment, and system design. A significant problem in the use of eye tracking for understanding perceptual and cognitive performance is that the data can be difficult to code, visualize, and analyze. In this presentation, we will present our methods for collecting and coding that data, as well as our plans for visualization and analysis of the data."},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/oco/public/values/cokwr"}],"gsx$date":{"$t":"1/19/2007"},"gsx$speaker":{"$t":"Melanie Wright"},"gsx$affiliation":{"$t":"Human Simulation and Patient Safety Center"},"gsx$title":{"$t":"Eye tracking in a human patient simulation environment: Data collection, coding, visualization, and analysis"},"gsx$abstract":{"$t":"One method of gaining insight into the underlying perceptual and cognitive performance in dynamic work environments is through the use of eye tracking measures. Eye tracking equipment can monitor the specific eye movements of practitioners. This information can be used to determine patterns of equipment use and information access. Researchers have used eye tracking techniques in a wide variety of environments for purposes such as comparing the scan patterns of experts and novices, evaluating menu scan behavior, or evaluating driver distraction. We are currently conducting a study evaluating various methods of assessing the differences between expert and novice performance in anesthesia. Among other performance measures, we are collecting eye tracking data while anesthesia providers treat two simulated anesthesia patients. The goal is to identify specific determinants of expertise in anesthesia. This information is expected to be useful for anesthesia training, assessment, and system design. A significant problem in the use of eye tracking for understanding perceptual and cognitive performance is that the data can be difficult to code, visualize, and analyze. In this presentation, we will present our methods for collecting and coding that data, as well as our plans for visualization and analysis of the data."},"gsx$video":{"$t":""},"gsx$videonotes":{"$t":""},"gsx$slides":{"$t":""},"gsx$slidesnotes":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/oco/public/values/cpzh4"},"updated":{"$t":"2015-12-11T12:54:10.655Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"1/26/2007"},"content":{"type":"text","$t":"speaker: Nikos Paragios, affiliation: Medical Imaging and Computer Vision Group, Ecole Centrale de Paris, title: Implicit Representations towards Content Extraction and Statistical Interpretation of Objects and Images, abstract: Statistical modeling, registration and segmentation of objects/classes of objects are among the most prominent applications in medical imaging and computer vision. The problem often consists of extracting a free-form or a constrained form object from an image. In the first case, image-based statistics are used to separate classes, while in the second case, prior knowledge either in the form of geometry or appearance is considered to better address the demand. Active contours and level set methods-aimed to minimize a functional defined over a curve or a surface-are quite popular techniques to perform such tasks. In this talk, we present an overview of our activities in these areas as well as their application in medical image analysis towards computer-aided diagnosis. First, we start with establishing the connection with explicit active contours and implicit surfaces, while next we present two novel image-based terms to address grouping that make better use of the boundaries and the regional statistics. Then, we discuss knowledge based-segmentation, a core problem in medical imaging where anatomy and domain knowledge can be used to infer more accurate solutions. First, we propose a novel registration technique that introduces the estimation of the uncertainty associated with the obtained solution. Then, we introduce novel dimensionality reduction techniques of increasing complexity starting from simple Gaussian models, linear sub-spaces, and variable bandwidth uncertainty-driven ICA and non-parametric statistical prior shape models. The models are used to introduce constraints in the segmentation process where uncertainties determine the local importance of the model. Promising results demonstrate the potentials or our method for surface registration, statistical modeling, and model-free and model-based segmentation. If time permits, the application of the same theoretical foundations to similar problems in the area of computer vision, like supervised texture segmentation, tracking, motion analysis, recognition, and facial animations, will be demonstrated."},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/oco/public/values/cpzh4"}],"gsx$date":{"$t":"1/26/2007"},"gsx$speaker":{"$t":"Nikos Paragios"},"gsx$affiliation":{"$t":"Medical Imaging and Computer Vision Group, Ecole Centrale de Paris"},"gsx$title":{"$t":"Implicit Representations towards Content Extraction and Statistical Interpretation of Objects and Images"},"gsx$abstract":{"$t":"Statistical modeling, registration and segmentation of objects/classes of objects are among the most prominent applications in medical imaging and computer vision. The problem often consists of extracting a free-form or a constrained form object from an image. In the first case, image-based statistics are used to separate classes, while in the second case, prior knowledge either in the form of geometry or appearance is considered to better address the demand. Active contours and level set methods-aimed to minimize a functional defined over a curve or a surface-are quite popular techniques to perform such tasks. In this talk, we present an overview of our activities in these areas as well as their application in medical image analysis towards computer-aided diagnosis. First, we start with establishing the connection with explicit active contours and implicit surfaces, while next we present two novel image-based terms to address grouping that make better use of the boundaries and the regional statistics. Then, we discuss knowledge based-segmentation, a core problem in medical imaging where anatomy and domain knowledge can be used to infer more accurate solutions. First, we propose a novel registration technique that introduces the estimation of the uncertainty associated with the obtained solution. Then, we introduce novel dimensionality reduction techniques of increasing complexity starting from simple Gaussian models, linear sub-spaces, and variable bandwidth uncertainty-driven ICA and non-parametric statistical prior shape models. The models are used to introduce constraints in the segmentation process where uncertainties determine the local importance of the model. Promising results demonstrate the potentials or our method for surface registration, statistical modeling, and model-free and model-based segmentation. If time permits, the application of the same theoretical foundations to similar problems in the area of computer vision, like supervised texture segmentation, tracking, motion analysis, recognition, and facial animations, will be demonstrated."},"gsx$video":{"$t":""},"gsx$videonotes":{"$t":""},"gsx$slides":{"$t":""},"gsx$slidesnotes":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/oco/public/values/cre1l"},"updated":{"$t":"2015-12-11T12:54:10.655Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"2/2/2007"},"content":{"type":"text","$t":"speaker: Michael Young, affiliation: Computer Science, NC State, title: Leveraging narrative concepts in the automatic construction of interactive experiences, abstract: The designers of computer games typically work quite hard at design time to construct user experiences that afford compelling story-based interactions. These experiences, while giving the illusion of dynamic and highly interactive content, are typically tightly constrained and play out in more or less the same manner across all users and across repeated playing sessions for the same user. Ongoing work at in the Liquid Narrative group at NC State University adapts models of narratives from a range of disciplines (e.g., cognitive psychology, linguistics, narrative theory, film theory) to design interactive 3D environments in which experiences are designed at run-time and managed to adapt to the dynamic and unexpected actions of users. These capabilities are relevant both to the construction of interactive entertainment applications as well as the design of new types of education and training applications. This talk will outline the relationship between the models used by other disciplines that the computational model used to procedurally generate interactive narrative experiences and provide some examples of the system's current capabilities as well as a discussion of the empirical evaluations to date. Time permitting, I'll also talk about current research looking at the use of linguistic models of discourse to control a 3D camera to create cinematic non-interactive narratives."},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/oco/public/values/cre1l"}],"gsx$date":{"$t":"2/2/2007"},"gsx$speaker":{"$t":"Michael Young"},"gsx$affiliation":{"$t":"Computer Science, NC State"},"gsx$title":{"$t":"Leveraging narrative concepts in the automatic construction of interactive experiences"},"gsx$abstract":{"$t":"The designers of computer games typically work quite hard at design time to construct user experiences that afford compelling story-based interactions. These experiences, while giving the illusion of dynamic and highly interactive content, are typically tightly constrained and play out in more or less the same manner across all users and across repeated playing sessions for the same user. Ongoing work at in the Liquid Narrative group at NC State University adapts models of narratives from a range of disciplines (e.g., cognitive psychology, linguistics, narrative theory, film theory) to design interactive 3D environments in which experiences are designed at run-time and managed to adapt to the dynamic and unexpected actions of users. These capabilities are relevant both to the construction of interactive entertainment applications as well as the design of new types of education and training applications. This talk will outline the relationship between the models used by other disciplines that the computational model used to procedurally generate interactive narrative experiences and provide some examples of the system's current capabilities as well as a discussion of the empirical evaluations to date. Time permitting, I'll also talk about current research looking at the use of linguistic models of discourse to control a 3D camera to create cinematic non-interactive narratives."},"gsx$video":{"$t":""},"gsx$videonotes":{"$t":""},"gsx$slides":{"$t":""},"gsx$slidesnotes":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/oco/public/values/chk2m"},"updated":{"$t":"2015-12-11T12:54:10.655Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"2/9/2007"},"content":{"type":"text","$t":"speaker: Daniela Rus, affiliation: CS and AI Lab, MIT, title: Programmable Matter with Self-reconfiguring Robots, abstract: We wish to create programmable matter by using smart modules capable of self-reconfiguration: hundreds of small modules autonomously organize and reorganize as geometric structures to best fit the terrain on which the robot has to move, the shape of the object the robot has to manipulate, or the sensing needs for the given task. Large collections of small robot modules actively organize as the most optimal geometric structure to perform useful coordinated work. A self-reconfiguring robot consists of a set of modules that can dynamically and autonomously reconfigure in a variety of shapes, to best fit the terrain, environment, and task. Self-reconfiguration leads to versatile robots that can support multiple modalities of locomotion, manipulation, and perception. This talk will discuss the challenges of creating programmable matter, ranging from designing hardware capable of self-reconfiguration, to developing distributed controllers and planners for such systems that are scalable, adaptive, and support real-time behavior. We will discuss a spectrum of mechanical and computational capabilities for such system and detail two recent robots developed for ground and underwater applications of programmable matter. Distributed Robotics Lab"},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/oco/public/values/chk2m"}],"gsx$date":{"$t":"2/9/2007"},"gsx$speaker":{"$t":"Daniela Rus"},"gsx$affiliation":{"$t":"CS and AI Lab, MIT"},"gsx$title":{"$t":"Programmable Matter with Self-reconfiguring Robots"},"gsx$abstract":{"$t":"We wish to create programmable matter by using smart modules capable of self-reconfiguration: hundreds of small modules autonomously organize and reorganize as geometric structures to best fit the terrain on which the robot has to move, the shape of the object the robot has to manipulate, or the sensing needs for the given task. Large collections of small robot modules actively organize as the most optimal geometric structure to perform useful coordinated work. A self-reconfiguring robot consists of a set of modules that can dynamically and autonomously reconfigure in a variety of shapes, to best fit the terrain, environment, and task. Self-reconfiguration leads to versatile robots that can support multiple modalities of locomotion, manipulation, and perception. This talk will discuss the challenges of creating programmable matter, ranging from designing hardware capable of self-reconfiguration, to developing distributed controllers and planners for such systems that are scalable, adaptive, and support real-time behavior. We will discuss a spectrum of mechanical and computational capabilities for such system and detail two recent robots developed for ground and underwater applications of programmable matter. Distributed Robotics Lab"},"gsx$video":{"$t":""},"gsx$videonotes":{"$t":""},"gsx$slides":{"$t":""},"gsx$slidesnotes":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/oco/public/values/ciyn3"},"updated":{"$t":"2015-12-11T12:54:10.655Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"2/16/2007"},"content":{"type":"text","$t":"speaker: Nicole Huff, affiliation: Cognitive Neuroscience, title: Relapse of conditioned fear in humans: laboratory studies to virtual reality, abstract: Fear to a specific stimuli, such as a snake or spider, can be greatly reduced with behavioral desensitization or exposure therapy in individuals with anxiety or phobic disorders, this is also termed extinction. However, following extinction a relapse of fear often occurs. Studies in humans and nonhuman animals have demonstrated that contextual cues play a role in determining when conditioned fears reappear following extinction. Specifically, a novel context, or one in which extinction did not occur, may renew the fear response. Currently, we are investigating the influence of context changes on the return of conditioned fear in healthy subjects in our laboratory at the Center for Cognitive Neuroscience at Duke. In parallel with this, we are developing a fear conditioning and renewal paradigm for the DiVE. Use of virtual contexts in the DiVE will allow us to have real-world environments and to make subtle manipulations in order to more accurately test our hypotheses. In addition, the DiVE scripts can be presented in the fMRI in order to assess which brain regions are involved in fear renewal. From these studies we hope to gain a better understanding of the mechanisms governing relapse of conditioned fear and to demonstrate the strength of virtual reality environments in both basic research and their potential clinical application."},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/oco/public/values/ciyn3"}],"gsx$date":{"$t":"2/16/2007"},"gsx$speaker":{"$t":"Nicole Huff"},"gsx$affiliation":{"$t":"Cognitive Neuroscience"},"gsx$title":{"$t":"Relapse of conditioned fear in humans: laboratory studies to virtual reality"},"gsx$abstract":{"$t":"Fear to a specific stimuli, such as a snake or spider, can be greatly reduced with behavioral desensitization or exposure therapy in individuals with anxiety or phobic disorders, this is also termed extinction. However, following extinction a relapse of fear often occurs. Studies in humans and nonhuman animals have demonstrated that contextual cues play a role in determining when conditioned fears reappear following extinction. Specifically, a novel context, or one in which extinction did not occur, may renew the fear response. Currently, we are investigating the influence of context changes on the return of conditioned fear in healthy subjects in our laboratory at the Center for Cognitive Neuroscience at Duke. In parallel with this, we are developing a fear conditioning and renewal paradigm for the DiVE. Use of virtual contexts in the DiVE will allow us to have real-world environments and to make subtle manipulations in order to more accurately test our hypotheses. In addition, the DiVE scripts can be presented in the fMRI in order to assess which brain regions are involved in fear renewal. From these studies we hope to gain a better understanding of the mechanisms governing relapse of conditioned fear and to demonstrate the strength of virtual reality environments in both basic research and their potential clinical application."},"gsx$video":{"$t":""},"gsx$videonotes":{"$t":""},"gsx$slides":{"$t":""},"gsx$slidesnotes":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/oco/public/values/ckd7g"},"updated":{"$t":"2015-12-11T12:54:10.655Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"2/23/2007"},"content":{"type":"text","$t":"speaker: Jeremy Block, affiliation: Biochemistry, title: Macromolecular VR: Home-Grown Graphics Development In The DiVE, abstract: As a student in the laboratory of David \u0026 Jane Richardson in the Dept. of Biochemistry, my dissertation work is centered around improving the accuracy, moreover quality, of protein and nucleic acid structures determined by nuclear magnetic resonance spectroscopy (NMR). A large component of this work dovetails with the field of molecular graphics, which the Richardson laboratory has been a significant contributor through the development of the kinemage graphics format and the Mage \u0026 KiNG graphics packages. Due to the inherent complexity of macromolecular structures, the fine detail in which we study them, and the added complication of multiple models used as representations for NMR structures, using a VR setup has been investigated and various tools developed in the last year. These tools, while developed for use by structural biologists, have applicability to other users here at Duke because of their fairly low entry barrier and open-source development in languages commonly used at Duke."},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/oco/public/values/ckd7g"}],"gsx$date":{"$t":"2/23/2007"},"gsx$speaker":{"$t":"Jeremy Block"},"gsx$affiliation":{"$t":"Biochemistry"},"gsx$title":{"$t":"Macromolecular VR: Home-Grown Graphics Development In The DiVE"},"gsx$abstract":{"$t":"As a student in the laboratory of David \u0026 Jane Richardson in the Dept. of Biochemistry, my dissertation work is centered around improving the accuracy, moreover quality, of protein and nucleic acid structures determined by nuclear magnetic resonance spectroscopy (NMR). A large component of this work dovetails with the field of molecular graphics, which the Richardson laboratory has been a significant contributor through the development of the kinemage graphics format and the Mage \u0026 KiNG graphics packages. Due to the inherent complexity of macromolecular structures, the fine detail in which we study them, and the added complication of multiple models used as representations for NMR structures, using a VR setup has been investigated and various tools developed in the last year. These tools, while developed for use by structural biologists, have applicability to other users here at Duke because of their fairly low entry barrier and open-source development in languages commonly used at Duke."},"gsx$video":{"$t":""},"gsx$videonotes":{"$t":""},"gsx$slides":{"$t":""},"gsx$slidesnotes":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/oco/public/values/clrrx"},"updated":{"$t":"2015-12-11T12:54:10.655Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"3/2/2007"},"content":{"type":"text","$t":"speaker: John Pormann, affiliation: CSEM, title: Computational Steering: Providing Interactivity to Running HPC Simulations, abstract: The current \"batch-mode\" mentality in high-performance computing reduces the efficiency of scientific researchers in that one can often view their data only after a simulation has run its full course. Thus, if a researcher is exploring a new area of their parameter space, they must then be willing to accept a certain number of \"bad\" outputs from their simulation runs. Once a simulation has entered a non-realistic range of parameters, e.g. a cell reaches a non-physiological state, a researcher cannot use their expertise to adjust where or how the simulation should progress, they can only kill the job and start over, potentially wasting hours or days of compute-time. With computational steering, we hope to more deeply engage the researcher in the progress of a running simulation, utilizing their expertise and knowledge to ensure that the simulation produces valid and useful results, and that the data that is produced and stored is only that which is meaningful. We have developed an HTTP-based computational steering framework in which a separate control-thread acts as an embedded web-server, attached to the memory space of the original simulation. A remote user can then connect to this web-server and read or alter the state of the running simulation. By using a separate control-thread, we hope to reduce the programming burden on the application programmer -- a dozen or so lines of additional code will provide a usable level of steering for most simulations."},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/oco/public/values/clrrx"}],"gsx$date":{"$t":"3/2/2007"},"gsx$speaker":{"$t":"John Pormann"},"gsx$affiliation":{"$t":"CSEM"},"gsx$title":{"$t":"Computational Steering: Providing Interactivity to Running HPC Simulations"},"gsx$abstract":{"$t":"The current \"batch-mode\" mentality in high-performance computing reduces the efficiency of scientific researchers in that one can often view their data only after a simulation has run its full course. Thus, if a researcher is exploring a new area of their parameter space, they must then be willing to accept a certain number of \"bad\" outputs from their simulation runs. Once a simulation has entered a non-realistic range of parameters, e.g. a cell reaches a non-physiological state, a researcher cannot use their expertise to adjust where or how the simulation should progress, they can only kill the job and start over, potentially wasting hours or days of compute-time. With computational steering, we hope to more deeply engage the researcher in the progress of a running simulation, utilizing their expertise and knowledge to ensure that the simulation produces valid and useful results, and that the data that is produced and stored is only that which is meaningful. We have developed an HTTP-based computational steering framework in which a separate control-thread acts as an embedded web-server, attached to the memory space of the original simulation. A remote user can then connect to this web-server and read or alter the state of the running simulation. By using a separate control-thread, we hope to reduce the programming burden on the application programmer -- a dozen or so lines of additional code will provide a usable level of steering for most simulations."},"gsx$video":{"$t":""},"gsx$videonotes":{"$t":""},"gsx$slides":{"$t":""},"gsx$slidesnotes":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/oco/public/values/cyevm"},"updated":{"$t":"2015-12-11T12:54:10.655Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"3/9/2007"},"content":{"type":"text","$t":"speaker: David Kidd, affiliation: Nescent, title: Visualizing Evolution in Space and Time, abstract: Evolution is a spatiotemporal process that occurs within a context of historical landscape, climate and community change. For the last 70 years cartographic overlay of evolutionary models onto maps has been the primary means of visual display. However, this is a limited form of graphical presentation that can struggle when presenting inherent complex evolutionary data that has two or three spatial and one temporal dimension, evolutionary divergence along branches, evolutionary convergence across branches, multiple species, historical geographical and climatic context, etc. I will describe the creation and visualization of 3D 'geophylogenies' in which trees and space are intimately linked as single data object. I will present visualizations of geophylogenies created in ArcSCENE with the GeoPhyloBuilder extension for ArcGIS and using Google Earth. EvoViz Wiki: http://evoviz.org"},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/oco/public/values/cyevm"}],"gsx$date":{"$t":"3/9/2007"},"gsx$speaker":{"$t":"David Kidd"},"gsx$affiliation":{"$t":"Nescent"},"gsx$title":{"$t":"Visualizing Evolution in Space and Time"},"gsx$abstract":{"$t":"Evolution is a spatiotemporal process that occurs within a context of historical landscape, climate and community change. For the last 70 years cartographic overlay of evolutionary models onto maps has been the primary means of visual display. However, this is a limited form of graphical presentation that can struggle when presenting inherent complex evolutionary data that has two or three spatial and one temporal dimension, evolutionary divergence along branches, evolutionary convergence across branches, multiple species, historical geographical and climatic context, etc. I will describe the creation and visualization of 3D 'geophylogenies' in which trees and space are intimately linked as single data object. I will present visualizations of geophylogenies created in ArcSCENE with the GeoPhyloBuilder extension for ArcGIS and using Google Earth. EvoViz Wiki: http://evoviz.org"},"gsx$video":{"$t":""},"gsx$videonotes":{"$t":""},"gsx$slides":{"$t":""},"gsx$slidesnotes":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/oco/public/values/cztg3"},"updated":{"$t":"2015-12-11T12:54:10.655Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"3/23/2007"},"content":{"type":"text","$t":"speaker: Owen Astrachan, affiliation: Computer Science, title: Visualizing Code, abstract: There are two ways to parse the phrase 'Visualizing Code', in this talk I'll address both of them. First, I'll talk about Tag Clouds and how this visualization has helped breathe new life into a stodgy old assignment we use in our first year programming classes. Then I will address two concerns: (1) can we visualize the commonalities in code written in languages that see uses in different contexts: C, C++, Perl, Python, Java, PHP and can we use these visualizations to understand the code; (2) can visualizations help with source code management. Knowledge of code will not be required to understand the talk. However, language bigots should bring their own muzzle. Interaction will be encouraged."},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/oco/public/values/cztg3"}],"gsx$date":{"$t":"3/23/2007"},"gsx$speaker":{"$t":"Owen Astrachan"},"gsx$affiliation":{"$t":"Computer Science"},"gsx$title":{"$t":"Visualizing Code"},"gsx$abstract":{"$t":"There are two ways to parse the phrase 'Visualizing Code', in this talk I'll address both of them. First, I'll talk about Tag Clouds and how this visualization has helped breathe new life into a stodgy old assignment we use in our first year programming classes. Then I will address two concerns: (1) can we visualize the commonalities in code written in languages that see uses in different contexts: C, C++, Perl, Python, Java, PHP and can we use these visualizations to understand the code; (2) can visualizations help with source code management. Knowledge of code will not be required to understand the talk. However, language bigots should bring their own muzzle. Interaction will be encouraged."},"gsx$video":{"$t":""},"gsx$videonotes":{"$t":""},"gsx$slides":{"$t":""},"gsx$slidesnotes":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/oco/public/values/d180g"},"updated":{"$t":"2015-12-11T12:54:10.655Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"4/6/2007"},"content":{"type":"text","$t":"speaker: Rochelle Schwartz-Bloom, affiliation: Pharmacology \u0026 Cancer Biology, title: Come Fly With Me: A 3D Animated Journey Into the Brain on Drugs, abstract: I will discuss the development of (and show) my 3D animated video called \"Animated Neuroscience and the Action of Nicotine, Cocaine, and Marijuana in the Brain\". Created over a 6 year period (just as Pentium chips came on the market), the 25 minute animation explains how neuronal transmission occurs, and how the addictive drugs work at the cellular level. It won the 1998 CINE Golden Eagle Award, and segments have been broadcast on programs such as WGBH's NOVA and an ABC Peter Jennings Special. The video is an excellent teaching tool for science and is now used nationwide in secondary and higher education."},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/oco/public/values/d180g"}],"gsx$date":{"$t":"4/6/2007"},"gsx$speaker":{"$t":"Rochelle Schwartz-Bloom"},"gsx$affiliation":{"$t":"Pharmacology \u0026 Cancer Biology"},"gsx$title":{"$t":"Come Fly With Me: A 3D Animated Journey Into the Brain on Drugs"},"gsx$abstract":{"$t":"I will discuss the development of (and show) my 3D animated video called \"Animated Neuroscience and the Action of Nicotine, Cocaine, and Marijuana in the Brain\". Created over a 6 year period (just as Pentium chips came on the market), the 25 minute animation explains how neuronal transmission occurs, and how the addictive drugs work at the cellular level. It won the 1998 CINE Golden Eagle Award, and segments have been broadcast on programs such as WGBH's NOVA and an ABC Peter Jennings Special. The video is an excellent teaching tool for science and is now used nationwide in secondary and higher education."},"gsx$video":{"$t":""},"gsx$videonotes":{"$t":""},"gsx$slides":{"$t":""},"gsx$slidesnotes":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/oco/public/values/d2mkx"},"updated":{"$t":"2015-12-11T12:54:10.655Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"4/13/2007"},"content":{"type":"text","$t":"speaker: Gil Bohrer, affiliation: Civil Engineering see a video of the talk, title: See the wind blow: virtual reality as a development and a science communication tool for large eddy simulations, abstract: The RAMS-Based Forest Large-Eddy Simulation (RAFLES) is a new numerical model that simulates turbulent wind flows in and above realistic, heterogeneous, 3-D, forest canopy domains. It is used to study the effects of tree/crown/scale heterogeneity of the forest environment on turbulence and fluxes to the atmospheric boundary layer. An AMIRA-based visualization interface for RAFLES was initially developed as a testing tool for the forest structure because standard computational-fluid-dynamics visualization tool lack a \"forest\" option. In operational mode, RAFLES generates 100 GB of data per simulation, which are affected by 10 MB of \"forest-structure\" data. DiVE visualization of the model results with the forest structures allows the viewer to detect patters in the flow and relate them to the structure. Once patterns are identified, automated numerical analysis of the data that summarize and test the significance of these patterns is possible. Presenting the models results in the DiVE provides an intuitive easy-to-understand environment. This facilitates the communication of the model's uniqueness and the significance of its results to other scientists across the broad a range of disciplines (biology, ecology, forestry, micrometeorology, remote sensing, and statistics) that are related to the scientific questions typically handled by RAFLES. The rich GUI of the AMIRA viewer allows immediate incorporation of viewer comments and allows cross-disciplinary flow of ideas that improve the visualization and the understanding of the complex forest-boundary-layer system."},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/oco/public/values/d2mkx"}],"gsx$date":{"$t":"4/13/2007"},"gsx$speaker":{"$t":"Gil Bohrer"},"gsx$affiliation":{"$t":"Civil Engineering see a video of the talk"},"gsx$title":{"$t":"See the wind blow: virtual reality as a development and a science communication tool for large eddy simulations"},"gsx$abstract":{"$t":"The RAMS-Based Forest Large-Eddy Simulation (RAFLES) is a new numerical model that simulates turbulent wind flows in and above realistic, heterogeneous, 3-D, forest canopy domains. It is used to study the effects of tree/crown/scale heterogeneity of the forest environment on turbulence and fluxes to the atmospheric boundary layer. An AMIRA-based visualization interface for RAFLES was initially developed as a testing tool for the forest structure because standard computational-fluid-dynamics visualization tool lack a \"forest\" option. In operational mode, RAFLES generates 100 GB of data per simulation, which are affected by 10 MB of \"forest-structure\" data. DiVE visualization of the model results with the forest structures allows the viewer to detect patters in the flow and relate them to the structure. Once patterns are identified, automated numerical analysis of the data that summarize and test the significance of these patterns is possible. Presenting the models results in the DiVE provides an intuitive easy-to-understand environment. This facilitates the communication of the model's uniqueness and the significance of its results to other scientists across the broad a range of disciplines (biology, ecology, forestry, micrometeorology, remote sensing, and statistics) that are related to the scientific questions typically handled by RAFLES. The rich GUI of the AMIRA viewer allows immediate incorporation of viewer comments and allows cross-disciplinary flow of ideas that improve the visualization and the understanding of the complex forest-boundary-layer system."},"gsx$video":{"$t":""},"gsx$videonotes":{"$t":""},"gsx$slides":{"$t":""},"gsx$slidesnotes":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/oco/public/values/cssly"},"updated":{"$t":"2015-12-11T12:54:10.655Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"4/20/2007"},"content":{"type":"text","$t":"speaker: Caroline Bruzelius, affiliation: Art, Art History and Visual Studies see a video of the talk, title: Build your own Gothic Cathedral, abstract: How can the teaching of an historical subject based in the twelfth and thirteenth centuries become a meaningful experience for students whose majors range from Biomedical Engineering to Art History to Economics? In order to do this, I developed a semester-long project in which students work in teams of three to create a cathedral and a fictional narrative. We train them on AutoCad to come up with a professional-looking design, and give prizes at the end of the term (the competition is a big part of the success of this project). I'll present this work with three students who took the course."},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/oco/public/values/cssly"}],"gsx$date":{"$t":"4/20/2007"},"gsx$speaker":{"$t":"Caroline Bruzelius"},"gsx$affiliation":{"$t":"Art, Art History and Visual Studies see a video of the talk"},"gsx$title":{"$t":"Build your own Gothic Cathedral"},"gsx$abstract":{"$t":"How can the teaching of an historical subject based in the twelfth and thirteenth centuries become a meaningful experience for students whose majors range from Biomedical Engineering to Art History to Economics? In order to do this, I developed a semester-long project in which students work in teams of three to create a cathedral and a fictional narrative. We train them on AutoCad to come up with a professional-looking design, and give prizes at the end of the term (the competition is a big part of the success of this project). I'll present this work with three students who took the course."},"gsx$video":{"$t":""},"gsx$videonotes":{"$t":""},"gsx$slides":{"$t":""},"gsx$slidesnotes":{"$t":""}},{"id":{"$t":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/oco/public/values/cu76f"},"updated":{"$t":"2015-12-11T12:54:10.655Z"},"category":[{"scheme":"http://schemas.google.com/spreadsheets/2006","term":"http://schemas.google.com/spreadsheets/2006#list"}],"title":{"type":"text","$t":"4/27/2007"},"content":{"type":"text","$t":"speaker: Zach Rosenthal, affiliation: Cognitive Behavioral Research and Treatment Program, title: Virtual Reality-Based Cue Exposure as an Adjunctive Intervention in Drug Counseling for Crack Cocaine Addiction: Treatment Development and Preliminary Findings, abstract: The aims of this project are to develop and manualize a virtual reality (VR) based cue exposure intervention for use in substance abuse counseling for crack cocaine addiction. The VR technology has been developed and refined using focus groups and an open clinical trial. A pilot randomized clinical trial comparing standard counseling to standard counseling augmented by the use of virtual reality-based cue exposure and cell-phone based cue extinction reminders (N \u003d 40) is being conducted in order to determine the acceptability and feasibility of this treatment to patients and therapists, and to obtain treatment outcome effect size estimates. In this presentation, the following topics will be discussed: (a) the rationale for using VR in behavioral treatments, (b) an overview and demonstration of the VR platform used in this study, and (c) preliminary data from an open trial with treating-seeking crack cocaine-dependent adults."},"link":[{"rel":"self","type":"application/atom+xml","href":"https://spreadsheets.google.com/feeds/list/1x_pPSVaxifb1EpqqKeQfUYyTkuVt0_7oYwqHcLhHDME/oco/public/values/cu76f"}],"gsx$date":{"$t":"4/27/2007"},"gsx$speaker":{"$t":"Zach Rosenthal"},"gsx$affiliation":{"$t":"Cognitive Behavioral Research and Treatment Program"},"gsx$title":{"$t":"Virtual Reality-Based Cue Exposure as an Adjunctive Intervention in Drug Counseling for Crack Cocaine Addiction: Treatment Development and Preliminary Findings"},"gsx$abstract":{"$t":"The aims of this project are to develop and manualize a virtual reality (VR) based cue exposure intervention for use in substance abuse counseling for crack cocaine addiction. The VR technology has been developed and refined using focus groups and an open clinical trial. A pilot randomized clinical trial comparing standard counseling to standard counseling augmented by the use of virtual reality-based cue exposure and cell-phone based cue extinction reminders (N \u003d 40) is being conducted in order to determine the acceptability and feasibility of this treatment to patients and therapists, and to obtain treatment outcome effect size estimates. In this presentation, the following topics will be discussed: (a) the rationale for using VR in behavioral treatments, (b) an overview and demonstration of the VR platform used in this study, and (c) preliminary data from an open trial with treating-seeking crack cocaine-dependent adults."},"gsx$video":{"$t":""},"gsx$videonotes":{"$t":""},"gsx$slides":{"$t":""},"gsx$slidesnotes":{"$t":""}}]}});